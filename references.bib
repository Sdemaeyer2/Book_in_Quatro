
@report{NAPLAN2014,
  title = {{{NAPLAN Achievement}} in {{Reading}}, {{Persuasive Writing}}, {{Language Conventions}} and {{Numeracy}}: {{National Report}} for 2014},
  author = {ACARA - Australian Curriculum, Assessment {and} Reporting Authority},
  date = {2014},
  institution = {{ACARA}},
  location = {{Sydney}},
  file = {/Users/demaeyer/Zotero/storage/UGMGQDSG/National_Assessment_Program_Literacy_and_Numeracy_national_report_for_2014.pdf}
}

@book{aeraStandards2014,
  title = {Standards for {{Educational}} and {{Psychological Testing}}},
  author = {{AERA APA \& NCME}},
  date = {2014},
  publisher = {{American Educational Research Association}},
  location = {{Washington D.C.}},
  url = {https://blackwells.co.uk/bookshop/product/Standards-for-Educational-and-Psychological-Testing-by-American-Educational-Research-Association-American-Psychological-Association-National-Council-on-Measurement-in-Education-Joint-Committee-on-Standards-for-Educational-and-Psychological-Testing-U-S-/9780935302356},
  urldate = {2021-12-25},
  abstract = {Testing standards that are a product of the American Educational Research Association, the American Psychological Association (APA), and the National Council on},
  isbn = {9780935302356},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/56EIKVD2/9780935302356.html}
}

@book{AEARA_Report2011,
  title = {Report and Recommendations for the Reauthorization of the Institute of Education Sciences},
  editor = {American Educational Research Association},
  date = {2011},
  publisher = {{American Educational Research Association}},
  location = {{Washington, D.C}},
  isbn = {978-0-935302-35-6},
  pagetotal = {60},
  keywords = {Education,Research},
  annotation = {OCLC: ocn826867074}
}

@article{andrews2007,
  title = {Compulsory Assessment Systems in the {{INCA}} Countries: {{Thematic Probe}}},
  author = {Andrews, Catherine and Brown, Ruth and Sargent, Claire},
  date = {2007},
  pages = {94},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/99LUFSHZ/Andrews e.a. - 2007 - Compulsory assessment systems in the INCA countrie.pdf}
}

@thesis{baartmanAssessing2008,
  type = {Doctoral Thesis},
  title = {Assessing the Assessment: {{Development}} and Use of Quality Criteria for {{Competence Assessment Programmes}}},
  shorttitle = {Assessing the Assessment},
  author = {Liesbeth Baartman},
  date = {2008-04-24},
  institution = {{Utrecht University}},
  isbn = {9789039347737},
  keywords = {Assessment,Competence Assessment Programmes}
}

@article{baartmanEvaluating2007,
  title = {Evaluating Assessment Quality in Competence-Based Education: {{A}} Qualitative Comparison of Two Frameworks},
  shorttitle = {Evaluating Assessment Quality in Competence-Based Education},
  author = {Baartman and Bastiaens and Kirschner and {van der Vleuten}},
  date = {2007},
  journaltitle = {Educational Research Review},
  shortjournal = {Educational Research Review},
  volume = {2},
  number = {2},
  pages = {114--129},
  issn = {1747-938X},
  doi = {10.1016/j.edurev.2007.06.001},
  url = {https://www.sciencedirect.com/science/article/pii/S1747938X0700019X},
  urldate = {2021-12-25},
  abstract = {Because learning and instruction are increasingly competence-based, the call for assessment methods to adequately determine competence is growing. Using just one single assessment method is not sufficient to determine competence acquisition. This article argues for Competence Assessment Programmes (CAPs), consisting of a combination of different assessment methods, including both traditional and new forms of assessment. To develop and evaluate CAPs, criteria to determine their quality are needed. Just as CAPs are combinations of traditional and new forms of assessment, criteria used to evaluate CAP quality should be derived from both psychometrics and edumetrics. A framework of 10 quality criteria for CAPs is presented, which is then compared to Messick's framework of construct validity. Results show that the 10-criterion framework partly overlaps with Messick's, but adds some important new criteria, which get a more prominent place in quality control issues in competence-based education.},
  langid = {english},
  keywords = {Assessment programmes,Competence-based education,Evaluation criteria,Quality control},
  file = {/Users/demaeyer/Zotero/storage/FY85WYBP/Baartman et al_2007_Evaluating assessment quality in competence-based education.pdf}
}

@article{baartmanWheel2006,
  title = {The Wheel of Competency Assessment: {{Presenting}} Quality Criteria for Competency Assessment Programs},
  shorttitle = {The Wheel of Competency Assessment},
  author = {Liesbeth Baartman and Theo Bastiaens and Paul Kirschner and Cees {van der Vleuten}},
  date = {2006-01-01},
  journaltitle = {Studies in Educational Evaluation},
  shortjournal = {Studies in Educational Evaluation},
  volume = {32},
  number = {2},
  pages = {153--170},
  issn = {0191-491X},
  doi = {10.1016/j.stueduc.2006.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0191491X06000228},
  urldate = {2021-12-25},
  abstract = {Instruction and learning are increasingly based on competencies, causing a call for assessment methods to adequately determine competency acquisition. Because competency assessment is such a complex endeavor, one single assessment method seems not to be sufficient. This necessitates Competency Assessment Programs (CAPS) that combine different methods, ranging from classical tests to recently developed assessment methods. However, many of the quality criteria used for classical tests cannot be applied to CAPS, since they use a combination of different methods rather than just one. This article presents a framework of 10 quality criteria for CAPS. An expert focus group was used to validate this framework. The results confirm the framework (9 out of 10 criteria) and expand it with 3 additional criteria. Based on the results, an adapted and layered new framework is presented.},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/5DUWD5Y9/Baartman et al_2006_The wheel of competency assessment.pdf}
}

@article{basturk2008,
  title = {Applying the Many‐facet {{Rasch}} Model to Evaluate {{PowerPoint}} Presentation Performance in Higher Education},
  author = {Basturk, Ramazan},
  date = {2008-08-01},
  journaltitle = {Assessment \& Evaluation in Higher Education},
  volume = {33},
  number = {4},
  pages = {431--444},
  publisher = {{Routledge}},
  issn = {0260-2938},
  doi = {10.1080/02602930701562775},
  url = {https://doi.org/10.1080/02602930701562775},
  urldate = {2021-12-25},
  abstract = {This study investigated the usefulness of the many‐facet Rasch model (MFRM) in evaluating the quality of performance related to PowerPoint presentations in higher education. The Rasch Model utilizes item response theory stating that the probability of a correct response to a test item/task depends largely on a single parameter, the ability of the person. MFRM extends this one‐parameter model to other facets of task difficulty, for example, rater severity, rating scale format, task difficulty levels. This paper specifically investigated presentation ability in terms of items/task difficulty and rater severity/leniency. First‐year science education students prepared and used the PowerPoint presentation software program during the autumn semester of the 2005–2006 school year in the ‘Introduction to the Teaching Profession’ course. The students were divided into six sub‐groups and each sub‐group was given an instructional topic, based on the content and objectives of the course, to prepare a PowerPoint presentation. Seven judges, including the course instructor, evaluated each group’s PowerPoint presentation performance using ‘A+ PowerPoint Rubric’. The results of this study show that the MFRM technique is a powerful tool for handling polytomous data in performance and peer assessment in higher education.},
  annotation = {\_eprint: https://doi.org/10.1080/02602930701562775},
  file = {/Users/demaeyer/Zotero/storage/XLTUA22R/Basturk_2008_Applying the many‐facet Rasch model to evaluate PowerPoint presentation.pdf;/Users/demaeyer/Zotero/storage/RK57W6H6/02602930701562775.html}
}

@article{bensimonMore2007,
  title = {Toward {{More Substantively Meaningful Automated Essay Scoring}}},
  author = {Ben-Simon, Anat and Bennett, Randy Elliot},
  date = {2007-08-01},
  journaltitle = {The Journal of Technology, Learning and Assessment},
  volume = {6},
  number = {1},
  issn = {1540-2525},
  url = {https://ejournals.bc.edu/index.php/jtla/article/view/1631},
  urldate = {2021-12-25},
  abstract = {This study evaluated a “substantively driven” method for scoring NAEP writing assessments automatically. The study used variations of an existing commercial program, e-rater®, to compare the performance of three approaches to automated essay scoring: a brute-empirical approach in which variables are selected and weighted solely according to statistical criteria, a hybrid approach in which a fixed set of variables more closely tied to the characteristics of good writing was used but the weights were still statistically determined, and a substantively driven approach in which a fixed set of variables was weighted according to the judgments of two independent committees of writing experts. The research questions concerned (1) the reproducibility of weights across writing experts, (2) the comparison of scores generated by the three automated approaches, and (3) the extent to which models developed for scoring one NAEP prompt generalize to other NAEP      prompts of the same genre. Data came from the 2002 NAEP Writing Online study and from the main NAEP 2002 writing assessment. Results showed that, in carrying out the substantively driven approach, experts initially assigned weights to writing dimensions that were highly similar across committees but that diverged from one another after committee 1 was shown the empirical weights for possible use in its judgments and committee 2 was not shown those weights. The substantively driven approach based on the judgments of committee 1 generally did not operate in a markedly different way from the brute empirical or hybrid approaches in most of the analyses conducted. In contrast, many consistent differences with those approaches were observed for the substantively driven approach based on the judgments of committee 2. This study suggests that empirical weights might provide a useful starting point for expert committees, with the understanding that the      weights be moderated only somewhat to bring them more into line with substantive considerations. Under such circumstances, the results may turn out to be reasonable, though not necessarily as highly related to human ratings as statistically optimal approaches would produce.},
  issue = {1},
  langid = {english},
  keywords = {automated essay scoring,writing assessment,writing scoring models},
  file = {/Users/demaeyer/Zotero/storage/TX72LY6J/Ben-Simon_Bennett_2007_Toward More Substantively Meaningful Automated Essay Scoring.pdf}
}

@article{biggsEnhancing1996,
  title = {Enhancing Teaching through Constructive Alignment},
  author = {Biggs, John},
  date = {1996-10-01},
  journaltitle = {Higher Education},
  shortjournal = {High Educ},
  volume = {32},
  number = {3},
  pages = {347--364},
  issn = {1573-174X},
  doi = {10.1007/BF00138871},
  url = {https://doi.org/10.1007/BF00138871},
  urldate = {2021-12-25},
  abstract = {Two lines of thinking are becoming increasingly important in higher educational practice. The first derives from constructivist learning theory, and the second from the instructional design literature. Constructivism comprises a family of theories but all have in common the centrality of the learner's activities in creating meaning. These and related ideas have important implications for teaching and assessment. Instructional designers for their part have emphasised alignment between the objectives of a course or unit and the targets for assessing student performance. “Constructive alignment” represents a marriage of the two thrusts, constructivism being used as a framework to guide decision-making at all stages in instructional design: in deriving curriculum objectives in terms of performances that represent a suitably high cognitive level, in deciding teaching/learning activities judged to elicit those performances, and to assess and summatively report student performance. The “performances of understanding” nominated in the objectives are thus used to systematically align the teaching methods and the assessment. The process is illustrated with reference to a professional development unit in educational psychology for teachers, but the model may be generalized to most units or programs in higher education.},
  langid = {english}
}

@book{biggsTeaching2011,
  title = {Teaching for Quality Learning at University: What the Student Does},
  shorttitle = {Teaching for Quality Learning at University},
  author = {Biggs, John B. and Tang, Catherine So-kum},
  date = {2011},
  series = {{{SRHE}} and {{Open University Press}} Imprint},
  edition = {4th edition},
  publisher = {{McGraw-Hill, Society for Research into Higher Education \& Open University Press}},
  location = {{Maidenhead, England New York, NY}},
  editora = {Society for Research into Higher Education},
  editoratype = {collaborator},
  isbn = {978-0-335-24275-7},
  langid = {english},
  pagetotal = {389},
  keywords = {Education / General,Education / Philosophy; Theory & Social Aspects},
  file = {/Users/demaeyer/Zotero/storage/R4KYBBRM/Biggs en Tang - 2011 - Teaching for quality learning at university what .pdf}
}

@incollection{birenbaumNew2003,
  title = {New {{Insights Into Learning}} and {{Teaching}} and {{Their Implications}} for {{Assessment}}},
  booktitle = {Optimising {{New Modes}} of {{Assessment}}: {{In Search}} of {{Qualities}} and {{Standards}}},
  author = {Birenbaum, Menucha},
  editor = {Segers, Mien and Dochy, Filip and Cascallar, Eduardo},
  date = {2003},
  series = {Innovation and {{Change}} in {{Professional Education}}},
  pages = {13--36},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/0-306-48125-1_2},
  url = {https://doi.org/10.1007/0-306-48125-1_2},
  urldate = {2021-12-25},
  isbn = {978-0-306-48125-3},
  langid = {english},
  keywords = {Computer Support Collaborative Learning,Educational Researcher,Folk Psychology,Formative Assessment,Information Communication Technology}
}


@book{brennanEducational2006,
  title = {Educational Measurement},
  editor = {Brennan, Robert L.},
  date = {2006},
  series = {Series on Higher Education},
  edition = {4. ed},
  publisher = {{American Council on Education [u.a.]}},
  location = {{New York}},
  isbn = {978-0-275-98125-9},
  langid = {english},
  pagetotal = {779}
}


@article{brennan1995,
  title = {Generalizability of {{Performance Assessments}}},
  author = {Brennan, Robert L. and Johnson, Eugene G .},
  date = {1995},
  journaltitle = {Educational Measurement: Issues and Practice},
  volume = {14},
  number = {4},
  pages = {9--12},
  issn = {1745-3992},
  doi = {10.1111/j.1745-3992.1995.tb00882.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3992.1995.tb00882.x},
  urldate = {2021-12-25},
  abstract = {How can the contributions of raters and tasks to error variance be estimated? Which source of error variance is usually greater? Are interrater coefficients adequate estimates of reliability? What other facets contribute to unreliability in performance assessments?},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3992.1995.tb00882.x},
  file = {/Users/demaeyer/Zotero/storage/CKNFV8HW/Brennan_Johnson_1995_Generalizability of Performance Assessments.pdf;/Users/demaeyer/Zotero/storage/R5UKI9ZI/Brennan_Johnson_1995_Generalizability of Performance Assessments.pdf;/Users/demaeyer/Zotero/storage/M364BKHW/j.1745-3992.1995.tb00882.html;/Users/demaeyer/Zotero/storage/W5LQDA8P/j.1745-3992.1995.tb00882.html}
}

@article{chapelleDoes2010,
  title = {Does an {{Argument-Based Approach}} to {{Validity Make}} a {{Difference}}?},
  author = {Chapelle, Carol A. and Enright, Mary K. and Jamieson, Joan},
  date = {2010},
  journaltitle = {Educational Measurement: Issues and Practice},
  volume = {29},
  number = {1},
  pages = {3--13},
  issn = {1745-3992},
  doi = {10.1111/j.1745-3992.2009.00165.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3992.2009.00165.x},
  urldate = {2021-12-25},
  abstract = {Drawing on experience between 2000 and 2007 in developing a validity argument for the high-stakes Test of English as a Foreign Language™ (TOEFL®), this paper evaluates the differences between the argument-based approach to validity as presented by Kane (2006) and that described in the 1999 AERA/APA/NCME Standards for Educational and Psychological Testing. Based on an analysis of four points of comparison—framing the intended score interpretation, outlining the essential research, structuring research results into a validity argument, and challenging the validity argument—we conclude that an argument-based approach to validity introduces some new and useful concepts and practices.},
  langid = {english},
  keywords = {claim,construct,high-stakes,inference,interpretive argument,Standards,TOEFL,validity argument},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3992.2009.00165.x},
  file = {/Users/demaeyer/Zotero/storage/TX29H96A/Chapelle et al_2010_Does an Argument-Based Approach to Validity Make a Difference.pdf;/Users/demaeyer/Zotero/storage/2Z59NSD7/j.1745-3992.2009.00165.html}
}

@article{chapelleValidity2012,
  title = {Validity Argument for Language Assessment: {{The}} Framework Is Simple…},
  shorttitle = {Validity Argument for Language Assessment},
  author = {Chapelle, Carol A.},
  date = {2012-01-01},
  journaltitle = {Language Testing},
  shortjournal = {Language Testing},
  volume = {29},
  number = {1},
  pages = {19--27},
  publisher = {{SAGE Publications Ltd}},
  issn = {0265-5322},
  doi = {10.1177/0265532211417211},
  url = {https://doi.org/10.1177/0265532211417211},
  urldate = {2021-12-25},
  file = {/Users/demaeyer/Zotero/storage/CTJNUGT8/Chapelle_2012_Validity argument for language assessment.pdf}
}

@article{childsMatrix2019,
  title = {Matrix {{Sampling}} of {{Items}} in {{Large-Scale Assessments}}},
  author = {Childs, Ruth and Jaciw, Andrew},
  date = {2019-11-23},
  journaltitle = {Practical Assessment, Research, and Evaluation},
  volume = {8},
  number = {1},
  issn = {1531-7714},
  doi = {10.7275/gwvh-4z51},
  url = {https://scholarworks.umass.edu/pare/vol8/iss1/16},
  file = {/Users/demaeyer/Zotero/storage/FXPB9IZJ/Childs en Jaciw - Matrix Sampling of Items in Large-Scale Assessment.pdf;/Users/demaeyer/Zotero/storage/TQ2J6EK7/16.html}
}

@incollection{cohenTest2006,
  title = {Test Administration, Security, Scoring, and Reporting},
  booktitle = {Educational {{Measurement}}},
  author = {Cohen, Allan and Wollack, James},
  editor = {Brennan, Robert L.},
  date = {2006-01-01},
  edition = {4},
  pages = {355--386},
  publisher = {{American Council on Education/Praeger}}
}

@article{cronbachConstruct1955,
  title = {Construct Validity in Psychological Tests},
  author = {Cronbach, Lee J. and Meehl, Paul E.},
  date = {1955},
  journaltitle = {Psychological Bulletin},
  volume = {52},
  number = {4},
  pages = {281--302},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1455},
  doi = {10.1037/h0040957},
  abstract = {"Construct validation was introduced in order to specify types of research required in developing tests for which the conventional views on validation are inappropriate. Personality tests, and some tests of ability, are interpreted in terms of attributes for which there is no adequate criterion. This paper indicates what sorts of evidence can substantiate such an interpretation, and how such evidence is to be interpreted." 60 references. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Construct Validity,Personality Measures,Psychometrics,Test Validity},
  file = {/Users/demaeyer/Zotero/storage/URT9S7BA/Cronbach_Meehl_1955_Construct validity in psychological tests.pdf;/Users/demaeyer/Zotero/storage/DMSFVWV9/1956-03730-001.html}
}

@book{cronbachPsychological1965,
  title = {Psychological Tests and Personnel Decisions},
  author = {Cronbach, Lee J and Gleser, Goldine C},
  date = {1965},
  publisher = {{University of Illinois Press}},
  location = {{Urbana}},
  isbn = {978-0-252-00003-4},
  langid = {english},
  annotation = {OCLC: 190689}
}

@incollection{cronbachTest1971,
  title = {Test Validation},
  booktitle = {Educational {{Measurement}}},
  author = {Cronbach, Lee},
  editor = {Thorndike, L.},
  date = {1971},
  edition = {2},
  pages = {443--507},
  publisher = {{American Council on Education/Praeger}},
  location = {{Washington D.C.}}
}

@article{crooksThreats1996,
  title = {Threats to the {{Valid Use}} of {{Assessments}}},
  author = {Crooks, Terry J. and Kane, Michael T. and Cohen, Allan S.},
  date = {1996-11-01},
  journaltitle = {Assessment in Education: Principles, Policy \& Practice},
  volume = {3},
  number = {3},
  pages = {265--286},
  publisher = {{Routledge}},
  issn = {0969-594X},
  doi = {10.1080/0969594960030302},
  url = {https://doi.org/10.1080/0969594960030302},
  urldate = {2021-12-26},
  abstract = {Validity is the most important quality of an assessment, but its evaluation is often neglected. The step‐by‐step approach suggested here provides structured guidance to validators of educational assessments. Assessment is depicted as a chain of eight linked stages: administration, scoring, aggregation, generalization, extrapolation, evaluation, decision and impact. Evaluating validity requires careful consideration of threats to validity associated with each link. Several threats are described and exemplified for each link. These sets of threats are intended to be illustrative rather than comprehensive. The chain model suggests that validity is limited by the weakest link, and that efforts to make other links particularly strong may be wasteful or even harmful. The chain model and list of threats is also shown to be valuable when planning assessments.},
  annotation = {\_eprint: https://doi.org/10.1080/0969594960030302},
  file = {/Users/demaeyer/Zotero/storage/LFR6SHYR/0969594960030302.html}
}

@report{curcinValidation2014,
  title = {A Validation Framework for Work-Based Observational Assessment in Vocational Qualifications},
  author = {Curcin, Milja and Boyle, Andrew and May, Tom and Rahman, Zeeshan},
  date = {2014},
  pages = {149},
  institution = {{Office of Qualifications and Examinations Regulation}},
  location = {{Coventry}},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/UQZSXAV8/Curcin e.a. - A validation framework for work-based observationa.pdf}
}

@book{darlinghammondBubble2014,
  title = {Beyond the Bubble Test: How Performance Assessments Support 21st Century Learning},
  shorttitle = {Beyond the Bubble Test},
  author = {Linda Darling-Hammond and Frank Adamson},
  date = {2014},
  edition = {First edition},
  publisher = {{Jossey-Bass \& Pfeiffer Imprints, Wiley}},
  location = {{San Francisco, CA}},
  isbn = {978-1-118-45618-7},
  pagetotal = {447},
  keywords = {Educational tests and measurements,United States}
}

@report{daveyPsychometric2015,
  title = {Psychometric Considerations for the next Generation of Performance Assessment. {{Princeton}}},
  author = {Davey, Tim and Ferrara, Steve and Holland, P.W. and Shavelson, Rich and Webb, Noreen M. and Wise, Lauress L.},
  date = {2015},
  institution = {{Educational Testing Service}},
  file = {/Users/demaeyer/Zotero/storage/KCDYAXL4/psychometric_considerations_white_paper.pdf}
}

@report{demaeyerHoe2016,
  type = {Eindrapport},
  title = {Hoe Zijn Competenties Grootschalig Te Toetsen? {{Ontwikkeling}} van Een Evaluatiematrix Voor Toetsprogramma’s En Een Inventarisatie van ‘Good Practices’.},
  author = {De Maeyer, Sven and Donche, Vincent and Vanhoof, Jan and Van Petegem, Peter and Coertjens, Liesje and De Groof, Jetje and Deneire, Alexia},
  date = {2016},
  institution = {{Departement Onderwijs}}
}

@report{dienstberoepsopleidingCompetentieleren2008,
  title = {Competentieleren: Een Gedachte-Experiment: {{Rapport}}},
  author = {{Dienst Beroepsopleiding}},
  date = {2008},
  institution = {{Dienst Beroepsopleiding, Departement Onderwijs en Vorming}},
  location = {{Brussel}}
}

@report{earu2014,
  title = {National {{Monitoring Study}} of {{Student Achievement}} ({{Wanangatia}} Te Putanga Tauira) - {{Health}} and {{Physical Education}} 2013},
  author = {EARU - Educational Assessment Research Unit \& NZCER - New Zealand Council for Educational Research},
  date = {2014},
  institution = {{Ministry of Education}},
  location = {{New Zealand}}
}

@article{eisnerUses1999,
  title = {The {{Uses}} and {{Limits}} of {{Performance Assessment}}},
  author = {Eisner, Elliot W.},
  date = {1999},
  journaltitle = {The Phi Delta Kappan},
  volume = {80},
  number = {9},
  eprint = {20439532},
  eprinttype = {jstor},
  pages = {658--660},
  publisher = {{Phi Delta Kappa International}},
  issn = {0031-7217}
}

@incollection{engelhardMonitoring2002,
  title = {Monitoring {{Raters}} in {{Performance Assessments}}},
  shorttitle = {Monitoring {{Raters}} in {{Performance Assessments}}},
  booktitle = {Large-Scale {{Assessment Programs}} for {{All Students}}},
  author = {Engelhard, George Jr.},
  editor = {Tindal, Gerald and Haladyna, Thomas M.},
  date = {2002},
  publisher = {{Routledge}},
  abstract = {As the number and variety of performance assessments increase in educational settings, itis  essential  to  monitor  and  evaluate  the  quality  of  ratings  obtained.  Any  assessmentsystem  that  goes  beyond  selected-response  (multiple-choice)  items  and  incorporatesconstructed-response  items  that  require  scoring  by  raters  must  have  procedures  formonitoring  rating  quality.  The  general  structure  of  rater-mediated  (RM)  assessmentsincludes  raters  judging  the  quality  of  an  examinee’s  response  to  a  task  designed  torepresent  the  construct  being  measured  using  a  rating  scale.  The  key feature  of  RMassessments is  that  the examinee’s responses (e.g.,  essays and portfolios)  become thestimuli that raters must interpret and evaluate to produce ratings. Although it may seemlike a trivial issue, it is very important that the measurement models used to evaluate RMassessments  are  indeed  models  of  rater  behavior,  performance,  and  response.  RMassessments do not provide direct information regarding examinee achievement becausethe  examinee’s  responses  must  be  mediated  and  interpreted  through  raters  to  obtainjudgments  about  examinee  achievement.  One  of  the  major  concerns  regarding  RMassessments is that raters bring a variety of potential response biases that may unfairlyaffect  their  judgments  regarding  the  quality  of  examinee  responses.  There  may be  avariety of construct-irrelevant components that may appear in RM assessment systemsrelated  to  response  biases  on  the  part  of  raters  that  impact  the  overall  validation  ofparticular  uses  and  interpretations  of  test  score  re-suits  within  the  context  of  stateassessment and accountability systems (Linn, chap. 2, this volume).},
  isbn = {978-1-4106-0511-5},
  pagetotal = {26}
}

@report{figelKey2007,
  title = {Key Competences for Lifelong Learning-{{European}} Reference Framework},
  author = {Figel, J.},
  date = {2007},
  institution = {{Office for Official Publications of the European Communities}},
  location = {{Luxemburg}}
}

@report{filmerCurriculum2010,
  title = {Curriculum and Assessment Models for the Home Language, Ages 11 – 17},
  author = {Filmer, Caroline and Sargent, Claire and O’Donnell, Sharon},
  date = {2010},
  pages = {85},
  institution = {{NFER}},
  location = {{Slough}},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/LDYP5UW4/Filmer e.a. - Curriculum and assessment models for the home lang.pdf}
}

@incollection{fitzpatrickPerformance1971,
  title = {Performance and {{Product Evaluation}}},
  booktitle = {Educational {{Measurement}}},
  author = {Fitzpatrick, R. and Morrison, E.},
  editor = {Thorndike, L.},
  date = {1971},
  edition = {2},
  pages = {443--507},
  publisher = {{American Council on Education/Praeger}},
  location = {{Washington D.C.}}
}

@report{gorinInherent2013,
  title = {Inherent {{Measurement Challenges}} in the {{Next Generation Science Standards}} for {{Both Formative}} and {{Summative Assessment}}},
  author = {Gorin, Joanna S and Mislevy, Robert J},
  date = {2013},
  pages = {39},
  institution = {{Educational Testing Service}},
  location = {{New Jersey}},
  url = {https://www.ets.org/Media/Research/pdf/gorin-mislevy.pdf},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/4YXBKZD2/Gorin en Mislevy - Inherent Measurement Challenges in the Next Genera.pdf}
}

@incollection{gulikersToetsen2017,
  title = {Toetsen van competenties},
  booktitle = {Toetsen in het hoger onderwijs},
  author = {Gulikers, Judith and van Benthum, Niek},
  editor = {van Berkel, Henk and Bax, Anneke and Joosten-ten Brinke, Desirée},

  date = {2017},
  pages = {227--239},
  publisher = {{Bohn Stafleu van Loghum}},
  location = {{Houten}},
  doi = {10.1007/978-90-368-1679-3_18},
  url = {https://doi.org/10.1007/978-90-368-1679-3_18},
  urldate = {2021-12-26},
  abstract = {Wat is competentietoetsing? Wanneer is het relevant en hoe ziet het er dan uit? Competentietoetsing geeft zicht op: (1) het vermogen van de student om kennis, vaardigheden en houdingen te integreren in adequaat professioneel handelen in beroepsrelevante situaties; (2) de bewuste beheersing van de competenties; en (3) het ontwikkelvermogen van de student. Om deze drie onderdelen te kunnen realiseren, is bij de inrichting van competentietoetsing allereerst een outputgerichte aanpak nodig. Dit start bij een analyse van wat de arbeidsmarkt van afgestudeerden vraagt. Deze analyse geeft input voor het ‘wat’ van de competentietoets (=de inhoud, ‘wat’ wil je van studenten zien/horen/lezen) om van daaruit vorm te gaan geven aan het ‘hoe’ (=de toetsvorm). Authentiek toetsen, het belang van beoordelingscriteria, integreren van formatief en summatief toetsen en het gebruik van rubrics komen ook aan bod als relevante aandachtspunten voor competentietoetsing.},
  isbn = {978-90-368-1679-3},
  langid = {dutch}
}

@inbook{haertelReliability2006,
  title = {Reliability},
  booktitle = {Educational {{Measurement}}},
  author = {Haertel, E.},
  date = {2006},
  edition = {4},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@article{hambletonSetting2000,
  title = {Setting {{Performance Standards}} on {{Complex Educational Assessments}}},
  author = {Hambleton, Ronald K. and Jaeger, Richard M. and Plake, Barbara S. and Mills, Craig},
  date = {2000-12-01},
  journaltitle = {Applied Psychological Measurement},
  shortjournal = {Applied Psychological Measurement},
  volume = {24},
  number = {4},
  pages = {355--366},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-6216},
  doi = {10.1177/01466210022031804},
  url = {https://doi.org/10.1177/01466210022031804},
  urldate = {2021-12-26},
  abstract = {Performance assessments have become popular in education and credentialing, and performance standards are common for interpreting and reporting scores. However, because of the unique characteristics of these assessments compared to multiple-choice tests (such as polytomous scoring), new and validstandard-setting methods are needed. Well-known standard-setting methods are no longer applicable. A number of promising methods for setting performance standards are described and their strengths and weaknesses are discussed. Suggestions for additional research are offered.},
  file = {/Users/demaeyer/Zotero/storage/2NH53L92/Hambleton et al_2000_Setting Performance Standards on Complex Educational Assessments.pdf}
}

@inbook{hambletonSetting2006,
  title = {Setting {{Performance Standards}}},
  booktitle = {Educational {{Measurement}}},
  author = {Hambleton, Ronald K.},
  date = {2006},
  edition = {4},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Pitoniak, B.S. and Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@article{heldsingerUsing2010,
  title = {Using the Method of Pairwise Comparison to Obtain Reliable Teacher Assessments},
  author = {Heldsinger and Humphry},
  date = {2010-08-01},
  journaltitle = {The Australian Educational Researcher},
  shortjournal = {Aust. Educ. Res.},
  volume = {37},
  number = {2},
  pages = {1--19},
  issn = {2210-5328},
  doi = {10.1007/BF03216919},
  url = {https://doi.org/10.1007/BF03216919},
  urldate = {2021-12-26},
  abstract = {Demands for accountability have seen the implementation of large scale testing programs in Australia and internationally. There is, however, a growing body of evidence to show that externally imposed testing programs do not have a sustained impact on student achievement. It has been argued that teacher assessment is more effective in raising student achievement levels. However, it is also often argued that teacher assessments are less reliable than the results of testing programs. This paper presents a study in which teachers judged writing scripts using the process of pairwise comparison to generate a scale. The analysis showed high internal consistency of the teacher judgements. The scale locations from pairwise comparisons were highly correlated with scale estimates for the same students from a large-scale testing program. The results demonstrate it is possible to efficiently obtain highly reliable and valid teacher judgements using the process of pairwise comparison. Reliability indices are also provided for a series of small-scale assessments that used the same methodology in a range of other domains. The results support the findings of the main study. The article discusses the benefits of using the method to supplement and validate results from large-scale testing programs.},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/53CQ2RF4/Heldsinger_Humphry_2010_Using the method of pairwise comparison to obtain reliable teacher assessments.pdf}
}

@article{heldsingerUsing2013,
  title = {Using Calibrated Exemplars in the Teacher-Assessment of Writing: An Empirical Study},
  shorttitle = {Using Calibrated Exemplars in the Teacher-Assessment of Writing},
  author = {Heldsinger, S. and Humphry},
  date = {2013},
  journaltitle = {Educational Research},
  volume = {55},
  number = {3},
  pages = {219--235},
  publisher = {{Routledge}},
  issn = {0013-1881},
  doi = {10.1080/00131881.2013.825159},
  url = {https://doi.org/10.1080/00131881.2013.825159},
  urldate = {2021-12-26},
  abstract = {Background: Many in education argue for the importance of incorporating teacher judgements in the assessment and reporting of student performance. Advocates of such an approach are cognisant, though, that obtaining a satisfactory level of consistency in teacher judgements poses a challenge. Purpose: This study investigates the extent to which the use of a two-stage method of assessment involving calibrated exemplars provides judgements from teachers that are consistent. Teachers were not given extensive training and moderation. We chose the assessment of early writing as a context to investigate the method as it is fundamental to students’ progress in schooling. Sample: Stage 1: Eleven teachers of four- to seven-year olds (kindergarten to year 2) were invited to collect their students’ performances. Sixty performances that represented the range of ability were selected from approximately 300 performances. Fifteen teachers from 12 schools made pairwise comparisons of performances. Stage 2: Fourteen teachers representing six schools plus the co-ordinator of the study participated in this stage of the exercise. Convenience sampling of teachers was employed. Design and method: Stage 1: The method of pairwise comparison was used to calibrate the performances of students by developing a performance scale. These performances were then used as exemplars, which are referred to here as calibrated exemplars. Stage 2: Teachers assessed student performances simply by judging which calibrated exemplar a performance was most alike. In a separate exercise, two experienced markers assessed another set of 118 writing performances using both (1) a criterion-based rubric and (2) the calibrated exemplars. Results: The two-staged process showed a level of consistency in teacher judgement-making. In addition, judgements made by experienced markers with the calibrated exemplars correlated well with judgements made using the criterion-based rubric. Conclusions: The findings suggest that using calibrated exemplars has potential as a method of teacher assessment in contexts where extensive training and moderation is not possible or desirable. Further research is needed to establish whether the findings generalise to the classroom context and whether consistency could be demonstrated on a large scale in this and other curriculum areas. Research is also needed to investigate whether the calibrated exemplars can be supported with qualitative information for use in formative assessment.},
  keywords = {accountability,calibrated exemplars,pairwise comparison,reliability,scoring methods,teacher assessment},
  annotation = {\_eprint: https://doi.org/10.1080/00131881.2013.825159},
  file = {/Users/demaeyer/Zotero/storage/JKADZ4DK/Heldsinger_Humphry_2013_Using calibrated exemplars in the teacher-assessment of writing.pdf;/Users/demaeyer/Zotero/storage/8R2UMDA8/00131881.2013.html}
}

@article{hillReliability2003,
  title = {Reliability of {{No Child Left Behind Accountability Designs}}},
  author = {Hill, Richard K. and DePascale, Charles A.},
  date = {2003},
  journaltitle = {Educational Measurement: Issues and Practice},
  volume = {22},
  number = {3},
  pages = {12--20},
  issn = {07311745, 17453992},
  doi = {10.1111/j.1745-3992.2003.tb00133.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1745-3992.2003.tb00133.x},
  urldate = {2021-12-26},
  langid = {english}
}

@inbook{hollandLinking2006,
  title = {Linking and Equation},
  booktitle = {Educational {{Measurement}}},
  author = {Holland, P.W. and DePascale, Charles A.},
  date = {2006},
  edition = {4},
  pages = {187--220},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@unpublished{hornsbyMisleading2012,
  title = {Misleading Everyone with Statistics},
  author = {Hornsby, D. and Wu, M.},
  date = {2012},
  url = {http://sydney.edu.au/education_social_work/news_events/resources/No_NAPLAN.pdf}
}

@book{johnsonAssessing2009,
  title = {Assessing Performance: Designing, Scoring, and Validating Performance Tasks},
  shorttitle = {Assessing Performance},
  author = {Johnson, Robert L. and Penny, James A. and Gordon, Belita},
  date = {2009},
  publisher = {{The Guilford Press}},
  location = {{New York}},
  isbn = {978-1-59385-988-6 978-1-59385-989-3},
  pagetotal = {355},
  keywords = {Educational evaluation,Employees,Evaluation,Performance,Rating of},
  annotation = {OCLC: ocn230204712}
}

@article{kaneValidating1999,
  title = {Validating {{Measures}} of {{Performance}}},
  author = {Kane and Crooks and Cohen},
  date = {1999},
  journaltitle = {Educational Measurement: Issues and Practice},
  volume = {18},
  number = {2},
  pages = {5--17},
  issn = {1745-3992},
  doi = {10.1111/j.1745-3992.1999.tb00010.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3992.1999.tb00010.x},
  urldate = {2021-12-27},
  abstract = {How can the validity of performance assessments be established? What type of logical argumentation is necessary to generalize from a particular assessment to the ability or abilities of interest?},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3992.1999.tb00010.x},
  file = {/Users/demaeyer/Zotero/storage/8FKARIM9/Kane et al_1999_Validating Measures of Performance.pdf;/Users/demaeyer/Zotero/storage/D2JZ76K5/j.1745-3992.1999.tb00010.html}
}

@article{kaneValidating2013,
  title = {Validating the {{Interpretations}} and {{Uses}} of {{Test Scores}}},
  author = {Kane, M.T. },
  date = {2013},
  journaltitle = {Journal of Educational Measurement},
  volume = {50},
  number = {1},
  pages = {1--73},
  issn = {1745-3984},
  doi = {10.1111/jedm.12000},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jedm.12000},
  urldate = {2021-12-27},
  abstract = {To validate an interpretation or use of test scores is to evaluate the plausibility of the claims based on the scores. An argument-based approach to validation suggests that the claims based on the test scores be outlined as an argument that specifies the inferences and supporting assumptions needed to get from test responses to score-based interpretations and uses. Validation then can be thought of as an evaluation of the coherence and completeness of this interpretation/use argument and of the plausibility of its inferences and assumptions. In outlining the argument-based approach to validation, this paper makes eight general points. First, it is the proposed score interpretations and uses that are validated and not the test or the test scores. Second, the validity of a proposed interpretation or use depends on how well the evidence supports the claims being made. Third, more-ambitious claims require more support than less-ambitious claims. Fourth, more-ambitious claims (e.g., construct interpretations) tend to be more useful than less-ambitious claims, but they are also harder to validate. Fifth, interpretations and uses can change over time in response to new needs and new understandings leading to changes in the evidence needed for validation. Sixth, the evaluation of score uses requires an evaluation of the consequences of the proposed uses; negative consequences can render a score use unacceptable. Seventh, the rejection of a score use does not necessarily invalidate a prior, underlying score interpretation. Eighth, the validation of the score interpretation on which a score use is based does not validate the score use.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jedm.12000},
  file = {/Users/demaeyer/Zotero/storage/AJW79UBU/Kane_2013_Validating the Interpretations and Uses of Test Scores.pdf;/Users/demaeyer/Zotero/storage/LIJW4MCQ/jedm.html}
}

@inbook{kaneValidation2006,
  title = {Validation},
  booktitle = {Educational {{Measurement}}},
  author = {Kane},
  date = {2006},
  edition = {4},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@book{kimbellEscape2007,
  title = {E-Scape Portfolio Assessment - Phase 2 Report},
  author = {Kimbell, Richard and Wheeler, Tony and Miller, Soo and Pollitt, Alastair},
  date = {2007},
  publisher = {{Goldsmiths}},
  location = {{London}},
  isbn = {978-1-904158-79-0},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/GPN8HS67/Kimbell - e-scape portfolio assessment - phase 2 report.pdf}
}

@book{kishStatistical2005,
  title = {Statistical {{Design}} for {{Research}}},
  author = {Kish, Leslie},
  date = {2005},
  url = {https://nbn-resolving.org/urn:nbn:de:101:1-20141021261},
  urldate = {2021-12-27},
  isbn = {978-0-471-72518-3 978-0-471-72519-0 978-0-471-69120-4 978-0-471-08359-7},
  langid = {english},
  annotation = {OCLC: 836883272}
}

@inbook{kolenScaling2006a,
  title = {Scaling and Norming},
  booktitle = {Educational {{Measurement}}},
  author = {Kolen},
  date = {2006},
  edition = {4},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@book{kolenTest2014,
  title = {Test Equating, Scaling, and Linking: Methods and Practices},
  shorttitle = {Test Equating, Scaling, and Linking},
  author = {Kolen and Brennan},
  date = {2014},
  series = {Statistics for Social Science and Public Policy},
  edition = {3d edition},
  publisher = {{Springer}},
  location = {{New York}},
  abstract = {Test equating methods are used with many standardized tests in education and psychology to ensure that scores from multiple test forms can be used interchangeably. In recent years, researchers from the education, psychology, and statistics communities have contributed to the rapidly growing statistical and psychometric methodologies used in test equating. This book provides an introduction to test equating which both discusses the most frequently used equating methodologies and covers many of the practical issues involved. This second edition expands upon the coverage of the first edition by providing a new chapter on test scaling and a second on test linking. Test scaling is the process of developing score scales that are used when scores on standardized tests are reported. In test linking, scores from two or more tests are related to one another. Linking has received much recent attention, due largely to investigations of linking similarly named tests from different test publishers or tests constructed for different purposes. The expanded coverage in the second edition also includes methodology for using polytomous item response theory in equating},
  isbn = {978-1-4939-0317-7},
  langid = {english}
}

@report{kuhlemeierBalans2013,
  title = {Balans van de Schrijfvaardigheid in Het Basis- En Speciaal Basisonderwijs 2. {{Uitkomsten}} van de Peiling in 2009 in Groep 5, Groep 8 En de Eindgroep van Het {{SBO}}},
  author = {Kuhlemeier, Hans and van Til, A. and Hemker, Bas and de Klijn, W. and Feenstra, H.},

  date = {2013},
  series = {{{PPON-reeks}}},
  number = {53},
  institution = {{Cito}},
  location = {{Arnhem}}
}

@article{kuhlemeierImpact2013,
  title = {Impact of {{Verbal Scale Labels}} on the {{Elevation}} and {{Spread}} of {{Performance Ratings}}},
  author = {Kuhlemeier, Hans and Hemker, Bas and van den Bergh, Huub},

  date = {2013-01-01},
  journaltitle = {Applied Measurement in Education},
  volume = {26},
  number = {1},
  pages = {16--33},
  publisher = {{Routledge}},
  issn = {0895-7347},
  doi = {10.1080/08957347.2013.739425},
  url = {https://doi.org/10.1080/08957347.2013.739425},
  urldate = {2021-12-27},
  abstract = {In recent years many countries have introduced authentic performance-based assessments in their national exam systems. Teachers’ ratings of their own candidates’ performances may suffer from errors of leniency and range restriction. The goal of this study was to examine the impact of manipulating the descriptiveness, balancedness, and polarity of the rating scales on the elevation and spread of the performance ratings. The study was conducted in the field setting of a (simulated) high-stakes national exam in Dutch pre-vocational education. A total of 55 teachers rated the performances of 652 candidates (aged ±16) on four authentic performance-based tasks. Multivariate multilevel analyses found the psychometric quality of the teachers’ performance ratings to be more favorable for positively unbalanced scales than for balanced scales. Positively unbalanced rating scales yielded the lowest (i.e., least generous) and most discriminative ratings. The descriptiveness and polarity of the rating scales were of lesser importance for the rating distributions. On the basis of the findings it was decided to introduce positively unbalanced scales in the national exams for pre-vocational education.},
  annotation = {\_eprint: https://doi.org/10.1080/08957347.2013.739425},
  file = {/Users/demaeyer/Zotero/storage/XXB9NGJE/Kuhlemeier et al_2013_Impact of Verbal Scale Labels on the Elevation and Spread of Performance Ratings.pdf;/Users/demaeyer/Zotero/storage/SDRAMAEF/08957347.2013.html}
}

@incollection{lanePerformance2006,
  title = {Performance {{Assessment}}},
  booktitle = {Educational {{Measurement}}},
  author = {Lane, Suzanne and Stone, C.},
  editor = {Brennan, Robert L.},
  date = {2006-01-01},
  edition = {4},
  pages = {387--432},
  publisher = {{American Council on Education/Praeger}}
}

@book{lanePerformance2010,
  title = {Performance Assessment: {{The}} State of the Art.},
  author = {Lane, Suzanne},
  date = {2010},
  series = {{{SCOPE Student Performance Assessment Series}}},
  publisher = {{Stanford University, Stanford Center of Opportunity Policy in Education}},
  location = {{Stanford, CA}},
  url = {https://edpolicy.stanford.edu/sites/default/files/publications/performance-assessment-state-art_1.pdf}
}

@inbook{lanePerformance2015a,
  title = {Performance {{Assessment}}: {{The State}} of the {{Art}}},
  shorttitle = {Performance {{Assessment}}},
  booktitle = {Beyond the {{Bubble Test}}},
  author = {S. Lane},
  date = {2015-09-16},
  pages = {131--184},
  publisher = {{John Wiley \& Sons, Inc.}},
  location = {{San Francisco}},
  doi = {10.1002/9781119210863.ch5},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781119210863.ch5},
  urldate = {2021-12-27},
  editor = {Darling-Hammond, Linda and Adamson, Frank},
  isbn = {978-1-119-21086-3 978-1-118-45618-7},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/AWSTAYFG/Lane - 2015 - Performance Assessment The State of the Art.pdf}
}

@inbook{lesterhuisComparative2017,
  title = {Comparative Judgement as a Promising Alternative to Score Competences},
  booktitle = {Innovative {{Practices}} for {{Higher Education Assessment}} and {{Measurement}}},
  author = {Lesterhuis and Verhavert and Coertjens and Donche and De Maeyer},
  date = {2017},
  pages = {119--136},
  doi = {10.4018/978-1-5225-0531-0.ch007},
  url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-85014150942&partnerID=MN8TOARS},
  bookauthor = {Cano, E. and Ion, G.}
}

@article{lesterhuisCompententies2015,
  title = {Compententies Kwaliteitsvol Beoordelen: Brengt Een Comparatieve Aanpak Soelaas?},
  author = {Lesterhuis and Donche and De Maeyer and van Daal and Van Gasse and Coertjens and Verhavert and Mortier and Coenen and Vlerick},
  date = {2015},
  journaltitle = {Tijdschrift voor Hoger Onderwijs},
  volume = {33},
  number = {2},
  pages = {55--67}
}

@article{linnComplex1991,
  title = {Complex, {{Performance-Based Assessment}}: {{Expectations}} and {{Validation Criteria}}},
  shorttitle = {Complex, {{Performance-Based Assessment}}},
  author = {Robert Linn and Eva Baker and Stephen B. Dunbar},
  date = {1991-11-01},
  journaltitle = {Educational Researcher},
  shortjournal = {Educational Researcher},
  volume = {20},
  number = {8},
  pages = {15--21},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X020008015},
  url = {https://doi.org/10.3102/0013189X020008015},
  urldate = {2021-12-27},
  abstract = {In recent years there has been an increasing emphasis on assessment results, as well as increasing concern about the nature of the most widely used forms of student assessment and uses that are made of the results. These conflicting forces have helped create a burgeoning interest in alternative forms of assessments, particularly complex, performance-based assessments. It is argued that there is a need to rethink the criteria by which the quality of educational assessments are judged, and a set of criteria that are sensitive to some of the expectations for performance-based assessments is proposed.},
  langid = {english}
}

@article{lissitzStandard2011,
  title = {Standard Setting in Complex Performance Assessments: {{An}} Approach Aligned with Cognitive Diagnostic Models},
  author = {Lissitz, Robert W and Li, Feifei},
  date = {2011},
  journaltitle = {Psychological Test and Assessment Modeling},
  volume = {53},
  number = {4},
  pages = {461--485},
  abstract = {With the increased interest in student-level diagnostic information from multiple performance assessments, it becomes possible to create multivariate classifications of knowledge, skills and abilities (KSAs). In this paper, a systematic, multivariate and non-compensating standard setting approach, called the cognitive analytical approach (CAA), is proposed for performance assessment with complex tasks.},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/A5MPRCMH/Lissitz en Li - Standard setting in complex performance assessment.pdf}
}

@article{lizzioAction2004,
  title = {Action {{Learning}} in {{Higher Education}}: An Investigation of Its Potential to Develop Professional Capability},
  shorttitle = {Action {{Learning}} in {{Higher Education}}},
  author = {Lizzio, Alf and Wilson, Keithia},
  date = {2004-08-01},
  journaltitle = {Studies in Higher Education},
  volume = {29},
  number = {4},
  pages = {469--488},
  publisher = {{Routledge}},
  issn = {0307-5079},
  doi = {10.1080/0307507042000236371},
  url = {https://doi.org/10.1080/0307507042000236371},
  urldate = {2021-12-27},
  abstract = {This study investigated the extent to which a course, designed using peer and action learning principles to function as an ‘on campus practicum’, can develop the professional capabilities of students. As part of their formal coursework, third year behavioural science students, functioning as ‘student consultants’, entered into a ‘client–consultant’ relationship with first and second year ‘student client’ groups. Both groups of students reported positive learning outcomes. Third year student consultants reported using less surface and more deep approaches to their learning in this course design than in concurrent courses taught along more conventional (i.e. lecture and tutorial) lines. Students also reported significantly greater development of meta‐adaptive skills (e.g. learning to learn) than in conventional teaching designs.},
  annotation = {\_eprint: https://doi.org/10.1080/0307507042000236371},
  file = {/Users/demaeyer/Zotero/storage/AXIJATP8/Lizzio _Wilson_2004_Action Learning in Higher Education.pdf;/Users/demaeyer/Zotero/storage/P2VB88A7/0307507042000236371.html}
}

@article{llosaBuilding2008,
  title = {Building and {{Supporting}} a {{Validity Argument}} for a {{Standards-Based Classroom Assessment}} of {{English Proficiency Based}} on {{Teacher Judgments}}},
  author = {Llosa, Lorena},
  date = {2008},
  journaltitle = {Educational Measurement: Issues and Practice},
  volume = {27},
  number = {3},
  pages = {32--42},
  issn = {1745-3992},
  doi = {10.1111/j.1745-3992.2008.00126.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-3992.2008.00126.x},
  urldate = {2021-12-27},
  abstract = {Using an argument-based approach to validation, this study examines the quality of teacher judgments in the context of a standards-based classroom assessment of English proficiency. Using Bachman's (2005) assessment use argument (AUA) as a framework for the investigation, this paper first articulates the claims, warrants, rebuttals, and backing needed to justify the link between teachers' scores on the English Language Development (ELD) Classroom Assessment and the interpretations made about students' language ability. Then the paper summarizes the findings of two studies—one quantitative and one qualitative—conducted to gather the necessary backing to support the warrants and, in particular, address the rebuttals about teacher judgments in the argument. The quantitative study examined the assessment in relation to another measure of the same ability—the California English Language Development Test—using confirmatory factor analysis of multitrait-multimethod data and provided evidence in support of the warrant that states that the ELD Classroom Assessment measures English proficiency as defined by the California ELD Standards. The qualitative study examined the processes teachers engaged in while scoring the classroom assessment using verbal protocol analysis. The findings of this study serve to support the rebuttals in the validity argument that state that there are inconsistencies in teachers' scoring. The paper concludes by providing an explanation for these seemingly contradictory findings using the AUA as a framework and discusses the implications of the findings for the use of standards-based classroom assessments based on teacher judgments.},
  langid = {english},
  keywords = {English learners,language proficiency,teacher judgments,validity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1745-3992.2008.00126.x},
  file = {/Users/demaeyer/Zotero/storage/SQKLI7SQ/Llosa_2008_Building and Supporting a Validity Argument for a Standards-Based Classroom.pdf;/Users/demaeyer/Zotero/storage/U4J4K3U5/j.1745-3992.2008.00126.html}
}

@book{luValidation2012,
  title = {A Validation Framework for Automated Essay Scoring Systems},
  author = {Lu, L.R.},
  date = {2012},
  series = {Unpublished Doctoral Dissertation},
  publisher = {{Faculty of Education, University of Wollongong}},
  location = {{Australia}}
}

@inbook{mazzeoMonitoring2006,
  title = {Monitoring {{Educational Progress}} with {{Group-Score Assessments}}},
  booktitle = {Educational {{Measurement}}},
  author = {Mazzeo, J. and Zieky, M.J.},
  date = {2006},
  edition = {4},
  pages = {681--699},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@article{messickInterplay1994,
  title = {The {{Interplay}} of {{Evidence}} and {{Consequences}} in the {{Validation}} of {{Performance Assessments}}},
  author = {Samuel Messick},
  date = {1994-03-01},
  journaltitle = {Educational Researcher},
  shortjournal = {Educational Researcher},
  volume = {23},
  number = {2},
  pages = {13--23},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X023002013},
  url = {https://doi.org/10.3102/0013189X023002013},
  urldate = {2021-12-27},
  abstract = {Authentic and direct assessments of performances and products are examined in the light of contrasting functions and purposes having implications for validation, especially with respect to the need for specialized validity criteria tailored for performance assessment. These include contrasts between performances and products, between assessment of performance per se and performance assessment of competence or other constructs, between structured and unstructured problems and response modes, and between breadth and depth of domain coverage. These distinctions are elaborated in the context of an overarching contrast between task-driven and construct-driven performance assessment. Rhetoric touting performance assessments because they eschew decomposed skills and decontextualized tasks is viewed as misguided, in that component skills and abstract problems have a legitimate place in pedagogy. Hence, the essence of authentic assessment must be sought elsewhere, that is, in the quest for complete construct representation. With this background, the concepts of “authenticity” and “directness” of performance assessment are treated as tantamount to promissory validity claims that they offset, respectively, the two major threats to construct validity, namely, construct underrepresentation and construct-irrelevant variance. With respect to validation, the salient role of both positive and negative consequences is underscored as well as the need, as in all assessment, for evidence of construct validity.},
  langid = {english}
}

@incollection{messickValidity1989,
  title = {Validity},
  booktitle = {Educational Measurement, 3rd Ed},
  author = {S. Messick},
  editor = {Linn, R.L.},
  date = {1989},
  series = {The {{American Council}} on {{Education}}/{{Macmillan}} Series on Higher Education},
  pages = {13--103},
  publisher = {{American Council on Education}},
  abstract = {amplifies . . . two basic points, namely, that validity is a unified though faceted concept and that validation is scientific inquiry  examine the nature and limitations of the traditional "types" of validity and how these conceptions have evolved over the years / present other ways of cutting evidence, and of reconfiguring forms of evidence, to highlight major facets of a unified validity conception  validity issues in relation to developments in the philosophy of science  evidential basis of test interpretation / construct validity  consequential basis of test interpretation  evidential basis of test use / meaning, relevance, and utility of test scores as requisites for test use  consequential basis of test use / social consequences of test use and their bearing on validity  evidence and ethics in test interpretation and use (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-0-02-922400-7},
  keywords = {Educational Measurement,Statistical Validity,Test Interpretation,Test Validity,Testing},
  file = {/Users/demaeyer/Zotero/storage/WEXYQMZI/1989-97348-002.html}
}

@inbook{messickValidity1996,
  title = {Validity of Performance Assessments},
  booktitle = {Technical Issues in Large-Scale Performance Assessment},
  author = {Messick},
  date = {1996},
  pages = {198--258},
  publisher = {{National Center for Education Statistics}},
  location = {{Washington D.C.}},
  editor = {Phillips, G.}
}

@article{mossCan1994,
  title = {Can {{There Be Validity Without Reliability}}?},
  author = {Moss, Pamela A.},
  date = {1994-03-01},
  journaltitle = {Educational Researcher},
  shortjournal = {Educational Researcher},
  volume = {23},
  number = {2},
  pages = {5--12},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X023002005},
  url = {https://doi.org/10.3102/0013189X023002005},
  urldate = {2021-12-27},
  abstract = {Reliability has traditionally been taken for granted as a necessary but insufficient condition for validity in assessment use. My purpose in this article is to illuminate and challenge this presumption by exploring a dialectic between psychometric and hermeneutic approaches to drawing and warranting interpretations of human products or performances. Reliability, as it is typically defined and operationalized in the measurement literature (e.g., American Educational Research Association [AERA], American Psychological Association, \& National Council on Measurement in Education, 1985; Feldt \& Brennan, 1989), privileges standardized forms of assessment. By considering hermeneutic alternatives for serving the important epistemological and ethical purposes that reliability serves, we expand the range of viable high-stakes assessment practices to include those that honor the purposes that students bring to their work and the contextualized judgments of teachers.},
  langid = {english}
}

@article{mulderConcept2007,
  title = {The Concept of Competence in the Development of Vocational Education and Training in Selected {{EU}} Member States: A Critical Analysis},
  shorttitle = {The Concept of Competence in the Development of Vocational Education and Training in Selected {{EU}} Member States},
  author = {Mulder, Martin and Weigel, Tanja and Collins, Kate},
  date = {2007-03-01},
  journaltitle = {Journal of Vocational Education \& Training},
  volume = {59},
  number = {1},
  pages = {67--88},
  publisher = {{Routledge}},
  issn = {1363-6820},
  doi = {10.1080/13636820601145630},
  url = {https://doi.org/10.1080/13636820601145630},
  urldate = {2021-12-30},
  abstract = {This contribution follows the descriptive review of Weigel, Mulder and Collins regarding the use of the competence concept in the development of vocational education and training in England, France, Germany and the Netherlands. The purpose of this contribution is to review the critical analyses brought forward by various authors in this field. This analysis also remarks on the most important theories and critiques on the use of the competence concept in the above‐mentioned states, The systems of vocational education within the four states covered in this study are: the National Vocational Qualifications in England, the approach to learning areas in Germany, the ETED and the bilan de compétences in France, and the implementation of competence‐based vocational education in the Netherlands, and these are the respective focal points for the critical assessments of the competence concept presented here. These critiques encompass such aspects as the lack of a coherent definition of the concept of competence, the lack of a one‐to‐one relationship between competence and performance, the misled notion that employing the concept of competence decreases the value of knowledge, the difficulties of designing competence‐based educational principles at the curriculum and instruction levels, the underestimation of the organizational consequences of competence‐based education, and the many problems in the field of competence assessment.},
  annotation = {\_eprint: https://doi.org/10.1080/13636820601145630},
  file = {/Users/demaeyer/Zotero/storage/TNQRY7IL/Mulder et al_2007_The concept of competence in the development of vocational education and.pdf;/Users/demaeyer/Zotero/storage/TNV8XS3C/13636820601145630.html}
}

@book{nationalresearchcouncilDeveloping2014,
  title = {Developing {{Assessments}} for the {{Next Generation Science Standards}}. {{Committee}} on {{Developing Assessments}} of {{Science Proficiency}} in {{K-12}}},
  author = {{National Research Council}},
  date = {2014},
  publisher = {{The National Academies Press}},
  location = {{Washington D.C.}}
}

@incollection{newhouseLiterature2013a,
  title = {Literature {{Review}} and {{Conceptual Framework}}},
  booktitle = {Digital {{Representations}} of {{Student Performance}} for {{Assessment}}},
  author = {Newhouse, Paul},
  editor = {Williams, P. John and Newhouse, C. Paul},
  date = {2013},
  pages = {9--28},
  publisher = {{SensePublishers}},
  location = {{Rotterdam}},
  doi = {10.1007/978-94-6209-341-6_2},
  url = {https://doi.org/10.1007/978-94-6209-341-6_2},
  urldate = {2021-12-27},
  abstract = {The aim of the research was to investigate the feasibility of using digital technologies to support performance assessment. As such the study connects with two main fields of research: performance assessment, and computer-supported assessment. However, clearly these are subsumed within the general field of assessment. While it will be assumed that the basic constructs within the field of assessment are known and apply perhaps it is useful to be reminded of this through a definition of assessment from Joughin (2009) and a statement of three pillars that Barrett (2005) suggests provide the foundation for every assessment.},
  isbn = {978-94-6209-341-6},
  langid = {english},
  keywords = {Digital Representation,Digital Technology,Electronic Portfolio,Student Work,Summative Assessment},
  file = {/Users/demaeyer/Zotero/storage/RFZL4P4M/Newhouse_2013_Literature Review and Conceptual Framework.pdf}
}

@article{newhouseUsing2011,
  title = {Using {{IT}} to Assess {{IT}}: {{Towards}} Greater Authenticity in Summative Performance Assessment},
  shorttitle = {Using {{IT}} to Assess {{IT}}},
  author = {Newhouse, C. Paul},
  date = {2011-02-01},
  journaltitle = {Computers \& Education},
  shortjournal = {Computers \& Education},
  volume = {56},
  number = {2},
  pages = {388--402},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2010.08.023},
  url = {https://www.sciencedirect.com/science/article/pii/S0360131510002514},
  urldate = {2021-12-27},
  abstract = {An applied Information Technology (IT) course that is assessed using pen and paper may sound incongruous but it is symptomatic of the state of high-stakes assessment in jurisdictions such as Western Australia. Whereas technology has permeated most aspects of modern life, including schooling, and more has been demanded of education systems in terms of outcomes and participation, methods of summative assessment have changed little and are seriously out of alignment with curriculum, pedagogy and the needs of individuals and society. This paper reports on an analysis of some of the data from a component of a study into the feasibility of using digital technologies to achieve greater authenticity in summative performance assessment in the Applied Information Technology (AIT) course in Western Australian secondary schools. In the first phase of the study a sample of 115 students completed a digital portfolio and a computer-based exam that were both externally assessed using online tools and by two methods of marking, with the results analysed using Rasch modelling software. A traditional analytical method and a comparative pairs method of marking were investigated. The study found that both the digital portfolio and computer-based exam were implemented without significant technical difficulty and were well accepted by the students and teachers. The work output in digital form was readily accessed from an online repository by external markers using a standard web browser. The two methods of marking provided highly reliable scores, with those from the comparative pairs method being the more reliable. A number of questions of validity and manageability were raised and the strengths and weaknesses of the two forms of assessment revealed. It was concluded that it was feasible to implement either form of assessment for high-stakes purposes, with a resulting improvement in alignment and authenticity.},
  langid = {english},
  keywords = {Computer education,Computer-based exam,Digital portfolio,Performance assessment,Summative assessment},
  file = {/Users/demaeyer/Zotero/storage/IL4X4LBU/Newhouse_2011_Using IT to assess IT.pdf}
}

@report{nferThematic2011,
  title = {Thematic Probe: {{Curriculum}} Specification in Seven Countries},
  author = {NFER},
  date = {2011},
  institution = {{National Foundation for Educational Research}},
  location = {{Slough}},
  file = {/Users/demaeyer/Zotero/storage/2CGMIG9Y/Curriculum_specification_in_seven_countries.pdf}
}

@incollection{pecheoneWhere2015,
  title = {Where {{We Are Now}}},
  booktitle = {Beyond the {{Bubble Test}}},
  author = {Pecheone, Raymond and Kahl, Stuart},
  date = {2015},
  pages = {53--91},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119210863.ch3},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119210863.ch3},
  urldate = {2021-12-27},
  abstract = {This chapter describes current state efforts to use performance assessment in large-scale state accountability systems. The states highlighted in the chapter present a window into promising assessment practices currently in place that can help shape the development of the next generation of assessment in the U.S. The chapter offers a description of current state assessment practices in four individual states, and four more that are part of the New England Common Assessments Program (NECAP), to illustrate the role performance assessments play in operational state accountability systems. It highlights three initiatives that focus on higher-order thinking skills, and college and workplace readiness: the College and Work Readiness Assessment (CWRA), the College Readiness and Performance Assessment System (C-PAS), and the Ohio Performance Assessment Pilot Project (OPAPP). The lessons learned described here are intended to guide the development and implementation of statewide performance assessment components of high quality and utility.},
  isbn = {978-1-119-21086-3},
  langid = {english},
  keywords = {C-PAS,CWRA,NECAP,OPAPP,performance assessments,state performance assessment models},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119210863.ch3},
  file = {/Users/demaeyer/Zotero/storage/5VM2L2K4/9781119210863.html}
}

@article{powersEffects1998,
  title = {Effects of {{Preexamination Disclosure}} of {{Essay Topics}}},
  author = {Powers, Donald E. and Fowles, Mary E.},
  date = {1998-04-01},
  journaltitle = {Applied Measurement in Education},
  volume = {11},
  number = {2},
  pages = {139--157},
  publisher = {{Routledge}},
  issn = {0895-7347},
  doi = {10.1207/s15324818ame1102_2},
  url = {https://doi.org/10.1207/s15324818ame1102_2},
  urldate = {2021-12-27},
  abstract = {To determine the effects on test performance and test validity of releasing essay topics before a writing examination, researchers asked 300 prospective graduate students to write 2 essays, 1 on a previously unseen topic and 1 on a topic that was made available about 2 weeks before the test. Although for each of the 4 essay topics studied, performance was slightly better when a topic had been disclosed, overall analyses did not reveal any statistically significant effect of disclosure on test performance. Consequently, there was no detectable effect of disclosure on the relation of test performance to several independent indicators of writing skill. Limitations of the study are discussed in terms of its generalizability to a high-stakes testing situation and the extent to which it addressed both the positive and the negative effects of disclosure.},
  annotation = {\_eprint: https://doi.org/10.1207/s15324818ame1102\_2},
  file = {/Users/demaeyer/Zotero/storage/26FHXTNN/Powers_Fowles_1998_Effects of Preexamination Disclosure of Essay Topics.pdf;/Users/demaeyer/Zotero/storage/LUDEWI8E/s15324818ame1102_2.html}
}

@article{powersWill1994,
  title = {Will {{They Think Less}} of {{My Handwritten Essay If Others Word Process Theirs}}? {{Effects}} on {{Essay Scores}} of {{Intermingling Handwritten}} and {{Word-Processed Essays}}},
  shorttitle = {Will {{They Think Less}} of {{My Handwritten Essay If Others Word Process Theirs}}?},
  author = {Powers, Donald E. and Fowles, Mary E. and Farnum, Marisa and Ramsey, Paul},
  date = {1994},
  journaltitle = {Journal of Educational Measurement},
  volume = {31},
  number = {3},
  eprint = {1435267},
  eprinttype = {jstor},
  pages = {220--233},
  publisher = {{[National Council on Measurement in Education, Wiley]}},
  issn = {0022-0655},
  abstract = {A study was undertaken to determine the effects on essay scores of intermingling handwritten and word-processed versions of student essays. A sample of examinees, each of whom had produced both a handwritten and a word-processed essay, was drawn from a larger sample of students who had participated in a pilot study of a new academic skills assessment battery. Students' original handwritten essays were converted to word-processed versions, and their original word-processed essays were converted to handwritten versions. Analyses revealed higher average scores for essays scored in the handwritten mode than for essays scored as word processed, regardless of the mode in which essays were originally produced. Several hypotheses were advanced to explain the discrepancies between scores on handwritten and word-processed essays. The training of essay readers was subsequently modified on the basis of these hypotheses, and the experiment was repeated using the modified training with a new set of readers.},
  file = {/Users/demaeyer/Zotero/storage/9VUTCIJS/Powers et al_1994_Will They Think Less of My Handwritten Essay If Others Word Process Theirs.pdf}
}

@article{prodromouBackwash1995,
  title = {The Backwash Effect: From Testing to Teaching},
  shorttitle = {The Backwash Effect},
  author = {Prodromou, Luke},
  date = {1995-01-01},
  journaltitle = {ELT Journal},
  shortjournal = {ELT Journal},
  volume = {49},
  number = {1},
  pages = {13--25},
  issn = {0951-0893},
  doi = {10.1093/elt/49.1.13},
  url = {https://doi.org/10.1093/elt/49.1.13},
  urldate = {2021-12-27},
  abstract = {Teach: If you teach someone something you give them instructions so they know about it or how to do it; you make them think, feel or act in a new or different way; you explain or show students how to do something. (Collins' COBUILD Dictionary)Test: To find out how much someone knows by asking them questions. (Longman's Active Study Dictionary).‘Teach’ and ‘test’ are quite close together in a dictionary, but in testing we do different things from the things we do when we teach. This article assesses the concept of ‘backwash’ in language teaching, looks at the consequences of testing on teaching in a broad educational context, and suggests that ‘negative backwash’ makes good language teaching more difficult. The two processes of testing and teaching are considered to be necessary but distinct. A system is described for distinguishing between them which is then applied to developing classroom activities for examination preparation classes, to help teachers move from testing to teaching procedures.},
  file = {/Users/demaeyer/Zotero/storage/GDVE7L76/2924351.html}
}

@inbook{rubinPreface1996,
  title = {A Preface Relating Alternative Assessment, Test Fairness, and Assessment Utility to Communication},
  booktitle = {Large Scale Assessment of Oral Communication: {{K}}–12 and Higher Education},
  author = {Rubin, D.},
  date = {1996},
  pages = {1--4},
  publisher = {{Speech Communication Association.}},
  location = {{Annandale}},
  url = {https://files.eric.ed.gov/fulltext/ED399578.pdf},
  bookauthor = {Morreale, S. and Backlund, P.}
}

@inbook{schmeiserTest2006,
  title = {Test {{Development}}},
  booktitle = {Educational {{Measurement}}},
  author = {Schmeiser, C. and Welch, C.},
  date = {2006},
  edition = {4},
  pages = {307--354},
  publisher = {{Praeger Publishers}},
  location = {{Westport}},
  bookauthor = {Brennan, Robert L.},
  isbn = {0-275-98125-8 978-0-275-98125-9}
}

@article{shavelsonMeasurement2010,
  title = {On the Measurement of Competency},
  author = {Shavelson, Richard J.},
  date = {2010-07},
  journaltitle = {Empirical Research in Vocational Education and Training},
  shortjournal = {Empirical Res Voc Ed Train},
  volume = {2},
  number = {1},
  pages = {41--63},
  publisher = {{SpringerOpen}},
  issn = {1877-6345},
  doi = {10.1007/BF03546488},
  url = {https://ervet-journal.springeropen.com/articles/10.1007/BF03546488},
  urldate = {2021-12-27},
  abstract = {Across multiple societal sectors, demand is growing to measure individual and group competencies. This paper unpacks Hartig et al.’s (2008) competency definition as a complex ability construct closely related to real-life-situation performance to make it amenable to measurement. Unpacked following the assessment triangle (construct, observation, inference), competency measurement is exemplified by research from business, military and education sectors. Generalizability theory, a statistical theory for modeling and evaluating the dependability of competency scores, is applied to several of these examples. The paper then pulls together the threads into a general competency measurement model.},
  issue = {1},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/ITXJEDU8/Shavelson_2010_On the measurement of competency.pdf;/Users/demaeyer/Zotero/storage/FYAJFKUH/BF03546488.html}
}

@article{shavelsonNote1999,
  title = {Note on {{Sources}} of {{Sampling Variability}} in {{Science Performance Assessments}}},
  author = {Shavelson, Richard J. and Ruiz-Primo, Maria Araceli and Wiley, Edward W.},
  date = {1999},
  journaltitle = {Journal of Educational Measurement},
  volume = {36},
  number = {1},
  eprint = {1435323},
  eprinttype = {jstor},
  pages = {61--71},
  publisher = {{[National Council on Measurement in Education, Wiley]}},
  issn = {0022-0655},
  abstract = {In 1993, we reported in "Journal of Educational Measurement" that task-sampling variability was the Achilles' heel of science performance assessment. To reduce measurement error, tasks needed to be stratified before sampling, sampled in large number, or possibly both. However, Cronbach, Linn, Brennan, \& Haertel (1997) pointed out that a task-sampling interpretation of a large person × task variance component might be incorrect. Task and occasion sampling are confounded because tasks are typically given on only a single occasion. The person × task source of measurement error is then confounded with the pt × occasion source. If pto variability accounts for a substantial part of the commonly observed pt interaction, stratifying tasks into homogenous subsets--a cost-effective way of addressing task sampling variability--might not increase accuracy. Stratification would not address the pto source of error. Another conclusion reported in JEM was that only direct observation (DO) and notebook (NB) methods of collecting performance assessment data were exchangeable; computer simulation, short-answer, and multiple-choice methods were not. However, if Cronbach et al. were right, our exchangeability conclusion might be incorrect. After re-examining and reanalyzing data, we found support for Conbach et al. We concluded that large task-sampling variability was due to both the person × task interaction and person × task × occasion interaction. Moreover, we found that direct observation, notebook and computer simulation methods were equally exchangeable, but their exchangeability was limited by the volatility of student performances across tasks and occasions.},
  file = {/Users/demaeyer/Zotero/storage/BXC38UAH/Shavelson et al_1999_Note on Sources of Sampling Variability in Science Performance Assessments.pdf}
}

@article{shawFramework2012,
  title = {A Framework for Evidencing Assessment Validity in Large-Scale, High-Stakes International Examinations},
  author = {Shaw, Stuart and Crisp, Victoria and Johnson, Nat},
  date = {2012-05-01},
  journaltitle = {Assessment in Education: Principles, Policy \& Practice},
  volume = {19},
  number = {2},
  pages = {159--176},
  publisher = {{Routledge}},
  issn = {0969-594X},
  doi = {10.1080/0969594X.2011.563356},
  url = {https://doi.org/10.1080/0969594X.2011.563356},
  urldate = {2021-12-27},
  abstract = {It is important for educational assessment bodies to demonstrate how they are seeking to meet the demands of validity. The approach to validity taken here assumes a ‘consequentialist’ view where the appropriacy of the inferences made on the basis of assessment results is seen as central. This paper describes the development of a systematic approach to the collection of evidence that can support claims about validity for general qualifications. An operational framework was developed drawing on Kane (2006). The framework involves a list of inferences to be justified as indicated by a number of linked validation questions. For each question various data would be gathered to provide ‘evidence for validity’ and to identify any ‘threats to validity’. The structure is designed to be accessible for operational users. This paper describes the development of the proposed framework and the types of methods to be used to gather relevant evidence.},
  keywords = {A level,examinations,validation,validity},
  annotation = {\_eprint: https://doi.org/10.1080/0969594X.2011.563356},
  file = {/Users/demaeyer/Zotero/storage/AU9UZTS2/Shaw et al_2012_A framework for evidencing assessment validity in large-scale, high-stakes.pdf;/Users/demaeyer/Zotero/storage/TWSWGCXN/0969594X.2011.html}
}

@incollection{sireciPacking2009,
  title = {Packing and Unpacking Sources of Validity Evidence: {{History}} Repeats Itself Again},
  shorttitle = {Packing and Unpacking Sources of Validity Evidence},
  booktitle = {The Concept of Validity: {{Revisions}}, New Directions, and Applications},
  author = {Sireci, Stephen G.},
  date = {2009},
  pages = {19--37},
  publisher = {{IAP Information Age Publishing}},
  location = {{Charlotte, NC, US}},
  abstract = {Validity has taken on many different meanings over the years, and different "types" or "aspects" of validity have been proposed to help define and guide test validation. In this chapter, I trace some of the history of test validity theory and validation and describe validity nomenclature that has evolved over the years. The historical review focuses on the Standards for Educational and Psychological Testing and its predecessors. Although the concept of validity has been described in various ways, there has long been consensus that validity is not an inherent characteristic of a test and what we seek to validate are inferences derived from test scores. The argument-based approach to validity as articulated in the current Standards provides sound advice for documenting validity evidence and for evaluating the use of a test for a particular purpose. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-1-60752-227-0 978-1-60752-228-7},
  keywords = {Educational Measurement,Experimentation,History of Psychology,Professional Standards,Psychological Assessment,Terminology,Test Validity},
  file = {/Users/demaeyer/Zotero/storage/A5YBEN8A/2009-23060-002.html}
}

@incollection{stecherLooking2015,
  title = {Looking {{Back}}},
  booktitle = {Beyond the {{Bubble Test}}},
  author = {Stecher, Brian},
  date = {2015},
  pages = {15--52},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119210863.ch2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119210863.ch2},
  urldate = {2021-12-27},
  abstract = {Performance assessment that is judging student achievement on the basis of relatively unconstrained responses to relatively rich stimulus materials, gained increasing favor in the United States in the late 1980s and 1990s. The chapter begins with a definition of performance assessment and suggests ways to classify different types of performance tasks. The classification is based on: stimulus materials and response options, content knowledge and process skills, subject field, and large-scale testing. Then the chapter provides background information on large-scale testing to familiarize readers with key terms and concepts. A review of some recent performance assessments efforts in the United States follows, along with a summary of research on quality, impact, and burden of performance assessments used in large-scale K–12 achievement testing. The chapter concludes with a discussion of the relevance of performance assessment to contemporary standards-based educational accountability and offers recommendations to support effective use of this form of assessment.},
  isbn = {978-1-119-21086-3},
  langid = {english},
  keywords = {content knowledge,large-scale K–12 achievement testing,performance assessment,process skills,response options,standards-based educational accountability,stimulus materials,subject field},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119210863.ch2},
  file = {/Users/demaeyer/Zotero/storage/WBRZYPGN/9781119210863.html}
}

@article{steedleEvaluating2016,
  title = {Evaluating {{Comparative Judgment}} as an {{Approach}} to {{Essay Scoring}}},
  author = {Steedle, Jeffrey T. and Ferrara, Steve},
  date = {2016-07-02},
  journaltitle = {Applied Measurement in Education},
  shortjournal = {Applied Measurement in Education},
  volume = {29},
  number = {3},
  pages = {211--223},
  issn = {0895-7347, 1532-4818},
  doi = {10.1080/08957347.2016.1171769},
  url = {http://www.tandfonline.com/doi/full/10.1080/08957347.2016.1171769},
  urldate = {2021-12-27},
  langid = {english}
}

@article{stemlerOverview2001,
  title = {An Overview of Content Analysis},
  author = {Stemler, Steve},
  date = {2001},
  journaltitle = {Practical Assessment, Research, and Evaluation},
  number = {7},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/Z6FM-2E34},
  url = {https://scholarworks.umass.edu/pare/vol7/iss1/17/},
  urldate = {2021-12-27},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/ZHEPQF88/Stemler - An overview of content analysis.pdf}
}

@incollection{straetmansToetsen2014,
  title = {Toetsen met performance assessment methodieken.},
  booktitle = {Toetsen in het hoger onderwijs},
  author = {Straetmans, G.},
  editor = {van Berkel, Henk and Bax, Anneke and Joosten-ten Brinke, Desiree},
  date = {2014},
  publisher = {{Bohn Stafleu van Loghum}},
  isbn = {978-90-368-0238-3},
  langid = {dutch},
  annotation = {OCLC: 917164491}
}

@article{tanWhy2011,
  title = {Why {{Do Standardized Testing Programs Report Scaled Scores}}?},
  author = {Tan, Xuan and Michel, Rochelle},
  date = {2011},
  journaltitle = {ETS R\&D Connections},
  number = {16},
  pages = {6},
  langid = {english},
  file = {/Users/demaeyer/Zotero/storage/9T4I3TD5/Tan en Michel - 2011 - Why Do Standardized Testing Programs Report Scaled.pdf}
}

@book{toulminUses2003,
  title = {The Uses of Argument},
  author = {Toulmin, Stephen},
  date = {2003},
  edition = {Updated ed},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, U.K. ; New York}},
  isbn = {978-0-521-82748-5 978-0-521-53483-3},
  pagetotal = {247},
  keywords = {Logic,Reasoning}
}

@book{trilling21st2013,
  title = {21st Century Skills: Learning for Life in Our Times},
  shorttitle = {21st Century Skills},
  author = {Trilling, Bernie and Fadel, Charles},
  date = {2013},
  publisher = {{Jossey-Bass}},
  location = {{San Francisco, Calif.}},
  url = {http://rbdigital.oneclickdigital.com},
  urldate = {2021-12-30},
  abstract = {The new building blocks for learning in a complex worldThis important resource introduces a framework for 21st Century learning that maps out the skills needed to survive and thrive in a complex and connected world. 21st Century content includes the basic core subjects of reading, writing, and arithmetic-but also emphasizes global awareness, financial/economic literacy, and health issues. The skills fall into three categories: learning and innovations skills; digital literacy skills; and life and career skills. This book is filled with vignettes, international examples, and classroom samples that help illustrate the framework and provide an exciting view of twenty-first century teaching and learning. Explores the three main categories of 21st Century Skills: learning and innovations skills; digital literacy skills; and life and career skills Addresses timely issues such as the rapid advance of technology and increased economic competitionBased on a framework developed by the Partnership for 21st Century Skills (P21)The book contains a DVD with video clips of classroom teaching. For more information on the book visit www.21stcenturyskillsbook.com.},
  isbn = {978-0-470-55391-6},
  langid = {english},
  annotation = {OCLC: 864562139}
}

@book{vanberkelToetsen2014,
  title = {Toetsen in het hoger onderwijs},
  author = {van Berkel, Henk and Bax, Anneke and Joosten-ten Brinke, Desir??e},

  date = {2014},
  publisher = {{Bohn Stafleu van Loghum}},
  isbn = {978-90-368-0238-3},
  langid = {dutch},
  annotation = {OCLC: 917164491}
}

@article{vandaalValidity2019b,
  title = {Validity of Comparative Judgement to Assess Academic Writing: Examining Implications of Its Holistic Character and Building on a Shared Consensus},
  shorttitle = {Validity of Comparative Judgement to Assess Academic Writing},
  author = {{van Daal} and Lesterhuis and Coertjens and Donche and De Maeyer},

  date = {2019-01-02},
  journaltitle = {Assessment in Education: Principles, Policy \& Practice},
  volume = {26},
  number = {1},
  pages = {59--74},
  publisher = {{Routledge}},
  issn = {0969-594X},
  doi = {10.1080/0969594X.2016.1253542},
  url = {https://doi.org/10.1080/0969594X.2016.1253542},
  urldate = {2021-12-30},
  abstract = {Recently, comparative judgement has been introduced as an alternative method for scoring essays. Although this method is promising in terms of obtaining reliable scores, empirical evidence concerning its validity is lacking. The current study examines implications resulting from two critical assumptions underpinning the use of comparative judgement, namely: its holistic characteristic and how the final rank order reflects the shared consensus on what makes for a good essay. Judges’ justifications that underpin their decisions are qualitatively analysed to obtain insight into the dimensions of academic writing they take into account. The results show that most arguments are directly related to the competence description. However, judges also use their expertise in order to judge the quality of essays. Additionally, judges differ in terms of how they conceptualise writing quality, and regarding the extent to which they tap into their own expertise. Finally, this study explores diverging conceptualisation of misfitting judges.},
  keywords = {academic writing,assessment,Comparative judgement,validity},
  annotation = {\_eprint: https://doi.org/10.1080/0969594X.2016.1253542},
  file = {/Users/demaeyer/Zotero/storage/WYKQBCEZ/van Daal et al_2019_Validity of comparative judgement to assess academic writing.pdf;/Users/demaeyer/Zotero/storage/UIX348TE/0969594X.2016.html}
}

@article{vandervleutenAssessing2005a,
  title = {Assessing Professional Competence: From Methods to Programmes},
  shorttitle = {Assessing Professional Competence},
  author = {van Der Vleuten, Cees P M and Schuwirth, Lambert W T},

  date = {2005},
  journaltitle = {Medical Education},
  volume = {39},
  number = {3},
  pages = {309--317},
  issn = {1365-2923},
  doi = {10.1111/j.1365-2929.2005.02094.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2929.2005.02094.x},
  urldate = {2021-12-30},
  abstract = {Introduction We use a utility model to illustrate that, firstly, selecting an assessment method involves context-dependent compromises, and secondly, that assessment is not a measurement problem but an instructional design problem, comprising educational, implementation and resource aspects. In the model, assessment characteristics are differently weighted depending on the purpose and context of the assessment. Empirical and theoretical developments Of the characteristics in the model, we focus on reliability, validity and educational impact and argue that they are not inherent qualities of any instrument. Reliability depends not on structuring or standardisation but on sampling. Key issues concerning validity are authenticity and integration of competencies. Assessment in medical education addresses complex competencies and thus requires quantitative and qualitative information from different sources as well as professional judgement. Adequate sampling across judges, instruments and contexts can ensure both validity and reliability. Despite recognition that assessment drives learning, this relationship has been little researched, possibly because of its strong context dependence. Assessment as instructional design When assessment should stimulate learning and requires adequate sampling, in authentic contexts, of the performance of complex competencies that cannot be broken down into simple parts, we need to make a shift from individual methods to an integral programme, intertwined with the education programme. Therefore, we need an instructional design perspective. Implications for development and research Programmatic instructional design hinges on a careful description and motivation of choices, whose effectiveness should be measured against the intended outcomes. We should not evaluate individual methods, but provide evidence of the utility of the assessment programme as a whole.},
  langid = {english},
  keywords = {education,educational measurement/*methods,medical,professional competence/*standards,undergraduate/*methods/standards},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2929.2005.02094.x},
  file = {/Users/demaeyer/Zotero/storage/NV8IWBH7/Van Der Vleuten_Schuwirth_2005_Assessing professional competence.pdf;/Users/demaeyer/Zotero/storage/S9W3CPHZ/j.1365-2929.2005.02094.html}
}

@article{WilsonLorraine2014Tttt,
  title = {Teaching to the Test},
  author = {Wilson, Lorraine and Hornsby, David},
  date = {2014},
  journaltitle = {Practically primary},
  volume = {19},
  number = {2},
  pages = {41--42},
  publisher = {{Australian Literacy Educators' Association}},
  issn = {1324-5961},
  abstract = {In Australian schools from January until May in 2011 everyone 'did' persuasive writing because it was to be tested by NAPLAN. Teachers were frantic, spending huge amounts of time having students write persuasive texts. (The same thing was happening in 2012 when we wrote this.) Professional development on persuasive writing has been in high demand. Like other education consultants around Australia, we have been inundated with requests to lead such sessions.},
  copyright = {COPYRIGHT 2014 Australian Literacy Educators' Association},
  langid = {english},
  keywords = {Competency based education,Curricula,Curriculum planning,Education,Elementary school teachers,Elementary school teaching,English language,Forecasts and trends,Methods,Practice,School prose,Study and teaching,Teacher-student relationships}
}

@inproceedings{Wools2012TowardsAC,
  title = {Towards a Comprehensive Evaluation System for the Quality of Tests and Assessments},
  author = {Wools, Saskia},
  date = {2012}
}

@thesis{woolsAll2015,
  title = {All {{About Validity}} - {{An}} Evaluation System for the Quality of Educational Assessment.},
  author = {Wools, Saskia},
  date = {2015},
  institution = {{University of Twente}},
  location = {{Enschede}}
}

@book{woolsBeoordelingsinstrument2007,
	title = {Beoordelingsinstrument: Kwaliteit van competentie assessment},
	author = {Wools, Saskia and Sanders, P. and Roelofs, E.},
	year = {2007},
	date = {2007},
	publisher = {Cito},
	address = {Arnhem}
}


@report{NAGBTechnology2014,
  title = {Technology and {{Engineering Literacy Framework}} for the 2014 {{National Assessment}} of {{Educational Progress}}.},
  author = {National Assessment Governing Board},
  date = {2014},
  institution = {{National Assessment Governing Board}},
  location = {{Washington D.C.}}
}


@report{ncesNation2012,
  title = {The {{Nation}}’s {{Report Card}}: {{Science}} in {{Action}}: {{Hands-On}} and {{Interactive Computer Tasks From}} the 2009 {{Science Assessment}} ({{NCES}} 2012-468)},
  author = {NCES},
  date = {2012},
  institution = {{Institute of Education Sciences, U.S. Department of Education}},
  location = {{Washington D.C.}}
}


@article{weigelConcept2007,
  title = {The Concept of Competence in the Development of Vocational Education and Training in Selected {{EU}} Member States},
  author = {Weigel, Tanja and Mulder, Martin and Collins, Kate},
  date = {2007-03-01},
  journaltitle = {Journal of Vocational Education \& Training},
  volume = {59},
  number = {1},
  pages = {53--66},
  publisher = {{Routledge}},
  issn = {1363-6820},
  doi = {10.1080/13636820601145549},
  url = {https://doi.org/10.1080/13636820601145549},
  urldate = {2022-01-03},
  abstract = {This contribution reviews how four European countries—England, Germany, France and the Netherlands—use the concept of competence in the process of developing vocational education and training. Competence in England is set in the context of the National Vocational Qualifications; in Germany within action competence and the approach to learning areas; in France within the Emploi Type Etudié dans sa Dynamique (ETED, translated by Céreq as Typical Employment Studied in Its Dynamics) and the bilan de compétences; and, in the Netherlands, the concept is strongly linked to the development of a competence‐based qualification structure for senior secondary vocational education. The nature of this review is rather descriptive as it aims to present a comparison of the meaning and use of the concept of competence within these respective countries. It can be concluded that despite a certain amount of diversity, especially at the instrumental level, there is enough conceptual convergence within the four above‐mentioned countries to posit some common principles of competence and competence‐based VET. In a subsequent contribution, Mulder et al. (this issue) will present a more critical analysis of the literature on the competence concept and its use. To avoid unnecessary overlap, the list of references for this article will be included in that second critical literature review.},
  annotation = {\_eprint: https://doi.org/10.1080/13636820601145549},
  file = {/Users/demaeyer/Zotero/storage/2IZX8H9K/Weigel et al_2007_The concept of competence in the development of vocational education and.pdf;/Users/demaeyer/Zotero/storage/XJB5QMY2/13636820601145549.html}
}


@article{prodromouBackwash1995a,
  title = {The Backwash Effect: From Testing to Teaching},
  shorttitle = {The Backwash Effect},
  author = {Prodromou, Luke},
  date = {1995-01-01},
  journaltitle = {ELT Journal},
  shortjournal = {ELT Journal},
  volume = {49},
  number = {1},
  pages = {13--25},
  issn = {0951-0893},
  doi = {10.1093/elt/49.1.13},
  url = {https://doi.org/10.1093/elt/49.1.13},
  urldate = {2022-01-03},
  abstract = {Teach: If you teach someone something you give them instructions so they know about it or how to do it; you make them think, feel or act in a new or different way; you explain or show students how to do something. (Collins' COBUILD Dictionary)Test: To find out how much someone knows by asking them questions. (Longman's Active Study Dictionary).‘Teach’ and ‘test’ are quite close together in a dictionary, but in testing we do different things from the things we do when we teach. This article assesses the concept of ‘backwash’ in language teaching, looks at the consequences of testing on teaching in a broad educational context, and suggests that ‘negative backwash’ makes good language teaching more difficult. The two processes of testing and teaching are considered to be necessary but distinct. A system is described for distinguishing between them which is then applied to developing classroom activities for examination preparation classes, to help teachers move from testing to teaching procedures.},
  file = {/Users/demaeyer/Zotero/storage/QY95VR4N/2924351.html}
}





